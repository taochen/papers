{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red38\green38\blue38;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl280\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 The paper presents a method to learn the post-condition of every adaptaion action. The method is called online because\
it use only one data per time, update it without using all historical data as the existing method.\
\
this method seems like the reenforcement learning which is one of the implicit search method, but as all other RL method,\
it\'a0is\'a0restraicted\'a0by\'a0the\'a0condition/action\'a0paris.\
\
Bad\
\
what if the action/condition mapping is quite large how do we make decision about which one to adapt?\
}