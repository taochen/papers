{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 This paper proposes a BO approach to optimise cloud-based ML training. It has two contributions:\
\
1. Tune both the hyper parameter and cloud resource configuration\
2. Using a subsampling. Note that this sub sampling is not the sample of configuration, but the sample of data that used to train the ML model under target.\
\
Other than that, this is a very generic BO approach.\
\
It seems that the acquisition function is a probability (Entropy Search), and it consider aspiration similar to p1 and p2 as constraint (accuracy is p0, though). A CEA is used for acquisition instead of grid search.\
\
ES can consider both the gain and cost (training size rate)\
\
Bad\
\
The number of measurement if merely 44?}