{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 This is an interesting work that extends BOHB (Bayesian Optimization and Hyperband), such that the model is built as an ensemble of model trained with multiple fidelity level.\
\
The generalized product of experts (gPoE) for aggregating and weighting the surrogate model and the relative rank of configuration to build the loss function are interesting, may be useful for the other work.\
\
\
Note that it does not need to actively sample different fidelity, but only as the search proceeds, get the data for different fidelity and train the ensemble. The paper assumes that the number of budget unit (e.g., number of measurement or training epoch) represents the fidelity. This makes sense, as in BOHB and HB, the search starts with the low fidelity configurations and progress till the high fidelity ones. In that way, the data required to train the ensemble can be gradually collected.\
\
bad\
\
However, the model is not directly used to model the fidelity, but still the original surrogate model in BO.}