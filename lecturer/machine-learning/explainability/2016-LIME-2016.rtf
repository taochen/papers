{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 The proposed LIME aims to explain the quantitative model by providing information about interpretable feature, so that qualitative explanation can be achieved.\
\
key novelty is how to find representative instance underpinned by interpretable features.\
\
\
This is done by:\
\
1. Build interpretable features, e.g., a adaptable feature with different values, but only make it as binary (on and off)\
2. sample all the features such that their values can also be translated into binary ones, compare the predicted results between original model and the explainable model (that takes only binary solution), as the local fidelity \
4. find the set of interpretable features that max the different between local fidelity and model complexity\
5. provide these features, with how their value changes can influence the predicted results (serve as instances), such that one can make qualitative decision about the trustworthy of the model\
\
\
This is a bit different from SE where many things are qualitative in general, and we need quantitative explanation }